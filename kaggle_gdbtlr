https://blog.csdn.net/weixin_42691585/article/details/109337381
# 建模
# 下面训练三个模型对数据进行预测， 分别是LR模型， GBDT模型和两者的组合模型， 然后分别观察它们的预测效果， 对于不同的模型， 特征会有不同的处理方式如下：
# 1. 逻辑回归模型： 连续特征要归一化处理， 离散特征需要one-hot处理
# 2. GBDT模型： 树模型连续特征不需要归一化处理， 但是离散特征需要one-hot处理
# 3. LR+GBDT模型： 由于LR使用的特征是GBDT的输出， 原数据依然是GBDT进行处理交叉， 所以只需要离散特征one-hot处理



# 1 读取数据
import pandas as pd
df_train=pd.read_csv('./train.csv') 
df_test=pd.read_csv('./test.csv') 


# 2 数据预处理

# 2 数据预处理

# 2.1 去掉ID
df_train.drop(['Id'],axis=1,inplace=True)
df_test.drop(['Id'],axis=1,inplace=True)
# 2.2 test增加label
df_test['Label'] = -1
# 2.3 拼接整体数据，且处理空值为-1
data = pd.concat([df_train, df_test])
data.fillna(-1,inplace=True)
# 2.4 特征列分开处理
continuous_fea = ['I'+str(i+1) for i in range(13)]
category_fea = ['C'+str(i+1) for i in range(26)]


# 3 模型训练
# 3.1 逻辑回归
# 3.1.1 特征处理
# 连续特征归一化
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
for col in continuous_fea:  
    data[col]=scaler.fit_transform(data[col].values.reshape(-1,1))
    
# 离散特征one-hot 编码     
for col in category_fea:
    onehot_feats = pd.get_dummies( data[col],prefix=col)
    data.drop([col],axis=1,inplace=True)
    data=pd.concat([data,onehot_feats],axis=1)

# 3.1.2 拆分训练集和测试集
# 拆分X和Y
train = data[data['Label'] != -1]
target = train.pop('Label')
test = data[data['Label'] == -1]
test.drop(['Label'], axis=1, inplace=True)
#拆分训练集和测试集
from sklearn.model_selection import train_test_split
x_train,x_val,y_train,y_val = train_test_split(train,target,test_size=0.2, random_state=2020)

# 3.1.3 建立模型
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(x_train,y_train)

# 返回第i行第j列的概率。第0列为=0的概率，第1列为=1 点击的概率。
from sklearn.metrics import log_loss
tr_logloss=log_loss(y_train, lr.predict_proba(x_train)[:,1])
val_logloss=log_loss(y_val, lr.predict_proba(x_val)[:,1])

print('LR_tr_logloss:',tr_logloss)
print('LR_val_logoss:',val_logloss)

# 3.1.4 模型预测
y_pred=lr.predict_proba(x_val)[:,1]
# 查看钱10个点击的概率
print('LR_predict:',y_pred[:10]) 

# 查看精确率，召回率，F1-score
lr.coef_
lr.intercept_
# pred预测的连续值切分为0-1， 并且计算精确值，召回值，f1，支持数
import numpy as np
y_pred2=np.where(y_pred>0.5,1,0)
print(classification_report(y_val,y_pred2,target_names=['不点击','点击']))

# 输出ROC
from sklearn.metrics import roc_curve
fpr,tpr,thresholds = roc_curve(y_val,y_pred)
print(fpr,tpr,thresholds)

import matplotlib.pyplot as plt
plt.plot(fpr,tpr)
plt.show()

    
# 3.2 GDBT

# 3.2.1 特征处理
# 离散特征one-hot编码 同上
# 离散特征one-hot 编码     
for col in category_fea:
    onehot_feats = pd.get_dummies( data[col],prefix=col)
    data.drop([col],axis=1,inplace=True)
    data=pd.concat([data,onehot_feats],axis=1)
# 3.2.2 训练集和测试集分开 同上
# 划分数据集 同上
# 3.2.3 建立模型
import lightgbm as lgb
gbm = lgb.LGBMClassifier(boosting_type='gdbt',
                   objective='binary',
                   subsample=0.8, #数据采样
                   min_child_weight=0.5,
                   colsample_bytree=0.7, #特征采样
                   num_leaves=100,
                   max_depth=12,
                   learning_rate=0.01,
                   n_estimators=10000
                  )
gbm.fit(x_train,y_train,
       eval_set=[(x_train,y_train),(x_val,y_val)],
       eval_names=['train','val'],
       eval_metric='binary_logloss',
       early_stopping_rounds=100)

# 查看损失函数
from sklearn.metrics import log_loss
tr_logloss=log_loss(y_train, gbm.predict_proba(x_train)[:,1])
val_logloss=log_loss(y_val, gbm.predict_proba(x_val)[:,1])

print('GDBT_tr_logloss:',tr_logloss)
print('GDBT_val_logoss:',val_logloss)


# 3.2.4 模型预测
y_pred=gbm.predict_proba(x_val)[:,1]
# 查看钱10个点击的概率
print('GDBT_predict:',test[:10]) 

# 查看准确率和召回率
import numpy as np
y_pred2=np.where(y_pred>0.5,1,0)
print('GDBT 效果：\n'+classification_report(y_val,y_pred2,target_names=['不点击','点击']))

# 查看ROC
from sklearn.metrics import roc_curve
fpr,tpr,thresholds = roc_curve(y_val,y_pred)
print(fpr,tpr,thresholds)

import matplotlib.pyplot as plt
plt.plot(fpr,tpr)
plt.show()


# 3.3 LR+GDBT
# 读取数据
import pandas as pd
df_train=pd.read_csv('./train.csv') 
df_test=pd.read_csv('./test.csv') 

# test增加默认-1的标签
df_test['Label'] = -1

# 拼接数据
data = pd.concat([df_train,df_test])
data.fillna(-1,inplace=True)
# 去掉ID
data.drop(['Id'],axis=1,inplace=True)

continuous_fea = ['I'+str(i+1) for i in range(13)]
category_fea = ['C'+str(i+1) for i in range(26)]

# 3.3.1 特征处理
# 离散特征one-hot编码 同上
# 离散特征one-hot 编码     
for col in category_fea:
    onehot_feats = pd.get_dummies( data[col],prefix=col)
    data.drop([col],axis=1,inplace=True)
    data=pd.concat([data,onehot_feats],axis=1)
    
  # 把训练集和测试集分开
    train = data[data['Label'] != -1]
    target = train.pop('Label')
    test = data[data['Label'] == -1]
    test.drop(['Label'], axis=1, inplace=True)

    
# 拆分训练集和测试集
from sklearn.model_selection import train_test_split
x_train,x_val,y_train,y_val = train_test_split(train,target,test_size=0.2, random_state=2020)  


# 模型训练
import lightgbm as lgb
gbm = lgb.LGBMClassifier(objective='binary',
                            subsample=0.8,
                            min_child_weight=0.5,
                            colsample_bytree=0.7,
                            num_leaves=100,
                            max_depth=12,
                            learning_rate=0.01,
                            n_estimators=1000,
                             )
# 训练模型
gbm.fit(x_train, y_train,
            eval_set=[(x_train, y_train), (x_val, y_val)],
            eval_names=['train', 'val'],
            eval_metric='binary_logloss',
            early_stopping_rounds=100)

model = gbm.booster_ 
# train 和 test 进行预测，并且输出
gbdt_feats_train = model.predict(train, pred_leaf=True)
gbdt_feats_test = model.predict(test, pred_leaf=True)

# GBDT预测结果进行字段命名-特征输出
gbdtt_feats_name1 = ['gbdt_leaf_' + str(i)+'_0' for i in range(gbdt_feats_train.shape[1])]

df_train_gbdt_feats = pd.DataFrame(gbdt_feats_train,columns=gbdt_feats_name)
df_test_gbdt_feats = pd.DataFrame(gbdt_feats_test,columns=gbdt_feats_name)

# 拼接GDBT预测结果生成新的训练集
train = pd.concat([train,df_train_gbdt_feats],axis=1)
test = pd.concat([test,df_test_gbdt_feats],axis=1)

# 查看有多少条数据
train_len = train.shape[0] 
# 增加GDBT训练之后的数节点为新的特征
data = pd.concat([train,test])
del train
del test
gc.collect()


# 连续特征归一化
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
for col in continuous_fea:  
    data[col]=scaler.fit_transform(data[col].values.reshape(-1,1))
    
# 离散特征one-hot 编码     
for col in gbdt_feats_name:
    onehot_feats = pd.get_dummies( data[col],prefix=col)
    data.drop([col],axis=1,inplace=True)
    data=pd.concat([data,onehot_feats],axis=1)   

# 切分训练集和测试集
train = data[:train_len]
test = data[train_len:]

del data
gc.collect()
x_train, x_val, y_train, y_val = train_test_split(train, target, test_size = 0.3, random_state = 2018)

# LR 模型训练
lr = LogisticRegression()
lr.fit(x_train,y_train)

from sklearn.metrics import log_loss
tr_logloss=log_loss(y_train, lr.predict_proba(x_train)[:,1])
val_logloss=log_loss(y_val, lr.predict_proba(x_val)[:,1])

print('GBDT+LR_tr_logloss:',tr_logloss)
print('GBDT+LR_val_logoss:',val_logloss)

y_pred = lr.predict_proba(test)[:,1]
print(y_pred[:10])



# 4 模型评估

